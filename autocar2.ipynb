{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import newaxis\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from mlagents.envs import UnityEnvironment\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, env, n_actions, n_features, action_low=-1, action_high=1, reward_decay=0.99,\n",
    "                 actor_learning_rate=0.01, critic_learning_rate=0.01, learning_rate_decay=0.95,\n",
    "                 ):\n",
    "        self.env = env\n",
    "        self.state_size = n_features\n",
    "        self.action_size = n_actions\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.gamma = reward_decay   # discount rate\n",
    "        self.actor_model_set = True\n",
    "        self.critic_model_set = True\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate # often larger than actor_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = 0.2 # used to clip\n",
    "        self.entfact = 2e-2 # entropy factor, to encourage exploration\n",
    "        self.lam = 0.95 # gae factor\n",
    "        self.memory = [] # store (s, a, r) for one agent\n",
    "        self.agents = 5 # number of agents that collect memory\n",
    "        self.history = {} # store the memory for different agents\n",
    "        self.history['states'] = []\n",
    "        self.history['observations'] = []\n",
    "        self.history['actions'] = []\n",
    "        self.history['discounted_rs'] = []\n",
    "        self.history['advantages'] = []\n",
    "        self._construct_nets()\n",
    "        \n",
    "    def _construct_nets(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.tfs = tf.placeholder(tf.float32, [None, self.state_size], 'state')\n",
    "        self.obs = tf.placeholder(tf.float32, [None, 16, 40, 1], 'observation')\n",
    "\n",
    "        # critic\n",
    "        with tf.variable_scope('critic'):\n",
    "#             net = tf.layers.conv2d(self.obs,\n",
    "#                                    filters=10,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu)\n",
    "#             net = tf.nn.relu(tf.layers.batch_normalization(net))\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[2, 2], strides=2)\n",
    "#             net = tf.layers.conv2d(net,\n",
    "#                                    filters=32,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[3, 3], strides=3)\n",
    "            n_l1_c = 128\n",
    "            net = tf.reshape(self.obs, [-1, 16*40])\n",
    "            w1_c_ob = tf.get_variable('w1_c_ob', [16*40, n_l1_c])\n",
    "            w1_c_s = tf.get_variable('w1_c_s', [self.state_size, n_l1_c])\n",
    "            b1_c = tf.get_variable('b1_c', [1, n_l1_c])\n",
    "            net = tf.nn.relu(tf.matmul(net, w1_c_ob) + tf.matmul(self.tfs, w1_c_s) + b1_c)\n",
    "            net = tf.layers.dense(net, 32, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(net, 1)\n",
    "            self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "            self.closs = tf.reduce_mean(tf.square(self.tfdc_r - self.v))\n",
    "            self.ctrain_op = tf.train.AdamOptimizer(self.critic_learning_rate).minimize(self.closs)\n",
    "\n",
    "        # actor\n",
    "        pi, pi_params = self._build_anet('pi', trainable=True)\n",
    "        test_pi = tf.distributions.Normal(loc=pi.mean(), scale=tf.zeros_like(pi.stddev()))\n",
    "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)       # choosing action\n",
    "            self.sample_test = tf.squeeze(test_pi.sample(1), axis=0) # deterministic action in test\n",
    "        with tf.variable_scope('update_oldpi'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        self.tfa = tf.placeholder(tf.float32, [None, self.action_size], 'action')\n",
    "        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
    "        with tf.variable_scope('loss'):\n",
    "            with tf.variable_scope('surrogate'):\n",
    "                self.ratio = pi.prob(self.tfa) / (oldpi.prob(self.tfa)+1e-10)\n",
    "                surr = self.ratio * self.tfadv\n",
    "                surr2 = tf.clip_by_value(self.ratio, 1-self.epsilon, 1+self.epsilon) * self.tfadv\n",
    "                self.aloss = - tf.reduce_mean(tf.minimum(surr, surr2)) - self.entfact * tf.reduce_mean(pi.entropy())\n",
    "\n",
    "        with tf.variable_scope('atrain'):\n",
    "            self.atrain_op = tf.train.AdamOptimizer(self.actor_learning_rate).minimize(self.aloss, var_list=pi_params)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_anet(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "#             net = tf.layers.conv2d(self.obs,\n",
    "#                                    filters=5,\n",
    "#                                    kernel_size=[3, 3],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu,\n",
    "#                                    trainable=trainable)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[2, 2], strides=2)\n",
    "#             net = tf.layers.conv2d(net,\n",
    "#                                    filters=32,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu,\n",
    "#                                    trainable=trainable)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[3, 3], strides=3)\n",
    "            n_l1_a = 128\n",
    "            net = tf.reshape(self.obs, [-1, 16*40])\n",
    "            w1_a_ob = tf.get_variable('w1_a_ob', [16*40, n_l1_a], trainable=trainable)\n",
    "            w1_a_s = tf.get_variable('w1_a_s', [self.state_size, n_l1_a], trainable=trainable)\n",
    "            b1_a = tf.get_variable('b1_a', [1, n_l1_a], trainable=trainable)\n",
    "            net = tf.nn.relu(tf.matmul(net, w1_a_ob) + tf.matmul(self.tfs, w1_a_s) + b1_a)\n",
    "            net = tf.layers.dense(net, 32, tf.nn.relu, trainable=trainable)\n",
    "#             net = tf.layers.dense(net, 512, trainable=trainable)\n",
    "#             net = tf.nn.relu(tf.layers.batch_normalization(net))\n",
    "            mu = max(np.abs(self.action_low), np.abs(self.action_high)) * tf.layers.dense(net, self.action_size, tf.nn.tanh, trainable=trainable)\n",
    "            sigma = tf.layers.dense(net, self.action_size, tf.nn.softplus, trainable=trainable)\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "    \n",
    "    def choose_action(self, state, observation, train=True): # normal distribution\n",
    "        assert self.actor_model_set, 'actor model not set!'\n",
    "        state = state.reshape(1,1)\n",
    "        if train:\n",
    "            a = self.sess.run(self.sample_op, {self.tfs: state, self.obs: observation})[0]\n",
    "        else:\n",
    "            a = self.sess.run(self.sample_test, {self.tfs: state, self.obs: observation})[0]\n",
    "        return np.clip(a, self.action_low, self.action_high)\n",
    "    \n",
    "    def remember(self, state, observation, action, reward, next_state, next_observation):\n",
    "        self.memory += [[state[0], observation[0], action, reward, next_state[0], next_observation[0]]]\n",
    "    \n",
    "    def discount_rewards(self, rewards, gamma, value_next=0.0):\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        running_add = value_next\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            discounted_r[t] = running_add = running_add * gamma + rewards[t]\n",
    "        return discounted_r\n",
    "    \n",
    "    def process_memory(self):\n",
    "        memory = np.vstack(np.array(self.memory))\n",
    "        states = np.vstack(memory[:,0])\n",
    "        observations = np.vstack(memory[:,1]).reshape((-1,16,40,1))\n",
    "        actions = np.vstack(memory[:,2])\n",
    "        rewards = memory[:,3]\n",
    "        last_next_state = memory[:,4][-1]\n",
    "        last_next_observation = memory[:,5][-1]\n",
    "        discounted_ep_rs = self.discount_rewards(rewards, self.gamma)[:, newaxis]\n",
    "        value_estimates = self.sess.run(self.v, {self.tfs: states, self.obs: observations}).flatten()\n",
    "#         value_estimates = self.sess.run(self.v, {self.tfs: np.r_[states, last_next_state[newaxis, :]], \n",
    "#                                                  self.obs: np.r_[observations, last_next_observation[newaxis, :]]}).flatten()\n",
    "#         last_value_estimate = self.sess.run(self.v, {self.tfs: , self.obs: })[0]\n",
    "        value_estimates = np.append(value_estimates, 0)\n",
    "        delta_t = rewards + self.gamma * value_estimates[1:] - value_estimates[:-1]\n",
    "        advs = self.discount_rewards(delta_t, self.gamma * self.lam)[:, newaxis] #gae\n",
    "        last = states.shape[0] #min(500, states.shape[0])\n",
    "        self.history['states'] += [states[-last:]]\n",
    "        self.history['observations'] += [observations[-last:]]\n",
    "        self.history['actions'] += [actions[-last:]]\n",
    "        self.history['discounted_rs'] += [discounted_ep_rs[-last:]]\n",
    "        self.history['advantages'] += [advs[-last:]]\n",
    "        self.memory = [] # empty the memory\n",
    "    \n",
    "    def replay(self):\n",
    "        assert self.actor_model_set, 'model not set!'\n",
    "        assert self.critic_model_set, 'critic model not set!'\n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        \n",
    "        s = np.vstack(self.history['states'])\n",
    "        ob = np.vstack(self.history['observations'])\n",
    "        ac = np.vstack(self.history['actions'])\n",
    "        dc_r = np.vstack(self.history['discounted_rs'])\n",
    "        ad = np.vstack(self.history['advantages'])\n",
    "#         ad = (ad-ad.mean())/ad.std()\n",
    "        \n",
    "        for _ in range(10): # update K epochs\n",
    "            s, ob, ac, dc_r, ad = shuffle(s, ob, ac, dc_r, ad)\n",
    "            for l in range(s.shape[0]//self.batch_size):\n",
    "                start = l * self.batch_size\n",
    "                end = (l + 1) * self.batch_size\n",
    "                self.sess.run(self.atrain_op, {self.tfs: s[start:end, :], self.obs:ob[start:end, :], self.tfa: ac[start:end, :], self.tfadv: ad[start:end, :]})\n",
    "                self.sess.run(self.ctrain_op, {self.tfs: s[start:end, :], self.obs:ob[start:end, :], self.tfdc_r: dc_r[start:end, :]})\n",
    "            if s.shape[0] % self.batch_size != 0:\n",
    "                res = s.shape[0] % self.batch_size\n",
    "                self.sess.run(self.atrain_op, {self.tfs: s[-res:, :], self.obs:ob[-res:, :], self.tfa: ac[-res:, :], self.tfadv: ad[-res:, :]})\n",
    "                self.sess.run(self.ctrain_op, {self.tfs: s[-res:, :], self.obs:ob[-res:, :], self.tfdc_r: dc_r[-res:, :]})\n",
    "#         self.actor_learning_rate *= self.learning_rate_decay\n",
    "#         self.critic_learning_rate *= self.learning_rate_decay\n",
    "        \n",
    "        for key in self.history:\n",
    "            self.history[key] = [] # empty the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:Start training by pressing the Play button in the Unity Editor.\n",
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 2\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CarPlayer\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 1\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n",
      "Unity brain name: CarLearning\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 1\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of Brains: 2\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CarPlayer\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 1\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n",
      "Unity brain name: CarLearning\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space size (per agent): 1\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env_name = \"autocar2_tmp\"\n",
    "#file_name=env_name\n",
    "env = UnityEnvironment()\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[1]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edges(img, thr1=10, thr2=15):\n",
    "    imgBlur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    edges = cv2.Canny(np.uint8(imgBlur*255),thr1,thr2)/255\n",
    "    return edges.reshape(-1,*edges.shape,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showEdges(img, thr1=10, thr2=15):\n",
    "    imgBlur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    edges = cv2.Canny(np.uint8(imgBlur*255),thr1,thr2)/255\n",
    "    # gr = img[...,:3].dot([0.299, 0.587, 0.114])\n",
    "    plt.subplots(figsize=(15,15))\n",
    "    plt.subplot(131), plt.xticks([]), plt.yticks([]), plt.title('Original Image')\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(132), plt.xticks([]), plt.yticks([]), plt.title('Blurred Image')\n",
    "    plt.imshow(imgBlur)\n",
    "    plt.subplot(133), plt.xticks([]), plt.yticks([]), plt.title('Edges')\n",
    "    plt.imshow(edges, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 16, 40, 3)\n",
      "Agent observations look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACoCAYAAADw6BWzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a9a7a66cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAACECAYAAACJflpcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY6IFAAAAAIX9P2nnY+NyqYoeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a9a79e9b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment\n",
    "#env_info = env.reset(train_mode=False)[default_brain]\n",
    "env_info = env.reset()\n",
    "brainInfo = env_info['CarLearning']\n",
    "print(np.array(brainInfo.visual_observations).shape)\n",
    "#print(env_info.states)\n",
    "\n",
    "for observation in brainInfo.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])\n",
    "\n",
    "img = brainInfo.visual_observations[0][0]\n",
    "#plt.imshow(img)\n",
    "#plt.show()\n",
    "showEdges(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State is : [12.91762638]\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    for _ in range(10):\n",
    "        env_info = env.step([1,0])[default_brain]\n",
    "#     print(str((i+1)*10)+' steps')\n",
    "    img = env_info.visual_observations[0][0]\n",
    "    if i==0:\n",
    "        sky = img[:3,:]\n",
    "    else:\n",
    "        sky += img[:3,:]\n",
    "#     showEdges(img)\n",
    "sky /= 40\n",
    "print('State is :', env_info.vector_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAA8CAYAAACdKPrlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a9a79fc668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sky)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(env,\n",
    "                n_actions=2,\n",
    "                n_features=1,\n",
    "                actor_learning_rate=1e-5,\n",
    "                critic_learning_rate=2e-5\n",
    "                )\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_ppo.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_ppo.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1000 rewards: 1100.49\n",
      "\n",
      "finished learning!\n"
     ]
    }
   ],
   "source": [
    "# PPO\n",
    "n_episodes = 1000\n",
    "\n",
    "agent.saver.restore(agent.sess, \"model/model_ppo.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=True)[default_brain]\n",
    "    state = env_info.vector_observations\n",
    "    env_info.visual_observations[0][0][:3,:] = sky # pretend that we always see the sky above\n",
    "    observation = edges(env_info.visual_observations[0][0])\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state, observation)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        env_info.visual_observations[0][0][:3,:] = sky # pretend that we always see the sky above\n",
    "        next_observation = edges(env_info.visual_observations[0][0])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        agent.remember(state, observation, action, reward, next_state, next_observation)\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            agent.process_memory()\n",
    "            rewards += [r]\n",
    "            break\n",
    "    if (i_episode+1) % agent.agents == 0: # update every n_agent episodes\n",
    "        agent.replay()\n",
    "#     if (i_episode+1) % 100 == 0:\n",
    "#         agent.saver.save(agent.sess, \"model/model2_ppo.ckpt\");\n",
    "agent.saver.save(agent.sess, \"model/model2_ppo.ckpt\");\n",
    "print(\"\\n\")\n",
    "print(\"finished learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ppo\n",
    "n_episodes = 1\n",
    "\n",
    "test_rewards = []\n",
    "# agent.saver.restore(agent.sess, \"model/model2_ppo_good.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=False)[default_brain]\n",
    "    state = env_info.states\n",
    "    env_info.observations[0][0][:3,:] = sky # pretend that we always see the sky above\n",
    "    observation = edges(env_info.observations[0][0])\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state, observation)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        state = env_info.states\n",
    "        env_info.observations[0][0][:3,:] = sky\n",
    "        observation = edges(env_info.observations[0][0])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "print(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(test_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
