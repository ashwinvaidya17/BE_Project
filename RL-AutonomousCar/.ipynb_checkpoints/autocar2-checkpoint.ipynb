{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import newaxis\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from mlagents.envs import UnityEnvironment\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, env, n_actions, n_features, action_low=-1, action_high=1, reward_decay=0.99,\n",
    "                 actor_learning_rate=0.01, critic_learning_rate=0.01, learning_rate_decay=0.95,\n",
    "                 ):\n",
    "        self.env = env\n",
    "        self.state_size = n_features\n",
    "        self.action_size = n_actions\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.gamma = reward_decay   # discount rate\n",
    "        self.actor_model_set = True\n",
    "        self.critic_model_set = True\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate # often larger than actor_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = 0.2 # used to clip\n",
    "        self.entfact = 2e-2 # entropy factor, to encourage exploration\n",
    "        self.lam = 0.95 # gae factor\n",
    "        self.memory = [] # store (s, a, r) for one agent\n",
    "        self.agents = 5 # number of agents that collect memory\n",
    "        self.history = {} # store the memory for different agents\n",
    "        self.history['states'] = []\n",
    "        self.history['observations'] = []\n",
    "        self.history['actions'] = []\n",
    "        self.history['discounted_rs'] = []\n",
    "        self.history['advantages'] = []\n",
    "        self._construct_nets()\n",
    "        \n",
    "    def _construct_nets(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.tfs = tf.placeholder(tf.float32, [None, self.state_size], 'state')\n",
    "        self.obs = tf.placeholder(tf.float32, [None, 16, 40, 1], 'observation')\n",
    "\n",
    "        # critic\n",
    "        with tf.variable_scope('critic'):\n",
    "#             net = tf.layers.conv2d(self.obs,\n",
    "#                                    filters=10,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu)\n",
    "#             net = tf.nn.relu(tf.layers.batch_normalization(net))\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[2, 2], strides=2)\n",
    "#             net = tf.layers.conv2d(net,\n",
    "#                                    filters=32,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[3, 3], strides=3)\n",
    "            n_l1_c = 128\n",
    "            net = tf.reshape(self.obs, [-1, 16*40])\n",
    "            w1_c_ob = tf.get_variable('w1_c_ob', [16*40, n_l1_c])\n",
    "            w1_c_s = tf.get_variable('w1_c_s', [self.state_size, n_l1_c])\n",
    "            b1_c = tf.get_variable('b1_c', [1, n_l1_c])\n",
    "            net = tf.nn.relu(tf.matmul(net, w1_c_ob) + tf.matmul(self.tfs, w1_c_s) + b1_c)\n",
    "            net = tf.layers.dense(net, 32, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(net, 1)\n",
    "            self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "            self.closs = tf.reduce_mean(tf.square(self.tfdc_r - self.v))\n",
    "            self.ctrain_op = tf.train.AdamOptimizer(self.critic_learning_rate).minimize(self.closs)\n",
    "\n",
    "        # actor\n",
    "        pi, pi_params = self._build_anet('pi', trainable=True)\n",
    "        test_pi = tf.distributions.Normal(loc=pi.mean(), scale=tf.zeros_like(pi.stddev()))\n",
    "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)       # choosing action\n",
    "            self.sample_test = tf.squeeze(test_pi.sample(1), axis=0) # deterministic action in test\n",
    "        with tf.variable_scope('update_oldpi'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        self.tfa = tf.placeholder(tf.float32, [None, self.action_size], 'action')\n",
    "        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
    "        with tf.variable_scope('loss'):\n",
    "            with tf.variable_scope('surrogate'):\n",
    "                self.ratio = pi.prob(self.tfa) / (oldpi.prob(self.tfa)+1e-10)\n",
    "                surr = self.ratio * self.tfadv\n",
    "                surr2 = tf.clip_by_value(self.ratio, 1-self.epsilon, 1+self.epsilon) * self.tfadv\n",
    "                self.aloss = - tf.reduce_mean(tf.minimum(surr, surr2)) - self.entfact * tf.reduce_mean(pi.entropy())\n",
    "\n",
    "        with tf.variable_scope('atrain'):\n",
    "            self.atrain_op = tf.train.AdamOptimizer(self.actor_learning_rate).minimize(self.aloss, var_list=pi_params)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_anet(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "#             net = tf.layers.conv2d(self.obs,\n",
    "#                                    filters=5,\n",
    "#                                    kernel_size=[3, 3],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu,\n",
    "#                                    trainable=trainable)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[2, 2], strides=2)\n",
    "#             net = tf.layers.conv2d(net,\n",
    "#                                    filters=32,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu,\n",
    "#                                    trainable=trainable)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[3, 3], strides=3)\n",
    "            n_l1_a = 128\n",
    "            net = tf.reshape(self.obs, [-1, 16*40])\n",
    "            w1_a_ob = tf.get_variable('w1_a_ob', [16*40, n_l1_a], trainable=trainable)\n",
    "            w1_a_s = tf.get_variable('w1_a_s', [self.state_size, n_l1_a], trainable=trainable)\n",
    "            b1_a = tf.get_variable('b1_a', [1, n_l1_a], trainable=trainable)\n",
    "            net = tf.nn.relu(tf.matmul(net, w1_a_ob) + tf.matmul(self.tfs, w1_a_s) + b1_a)\n",
    "            net = tf.layers.dense(net, 32, tf.nn.relu, trainable=trainable)\n",
    "#             net = tf.layers.dense(net, 512, trainable=trainable)\n",
    "#             net = tf.nn.relu(tf.layers.batch_normalization(net))\n",
    "            mu = max(np.abs(self.action_low), np.abs(self.action_high)) * tf.layers.dense(net, self.action_size, tf.nn.tanh, trainable=trainable)\n",
    "            sigma = tf.layers.dense(net, self.action_size, tf.nn.softplus, trainable=trainable)\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "    \n",
    "    def choose_action(self, state, observation, train=True): # normal distribution\n",
    "        assert self.actor_model_set, 'actor model not set!'\n",
    "        state = state.reshape(1,1)\n",
    "        if train:\n",
    "            a = self.sess.run(self.sample_op, {self.tfs: state, self.obs: observation})[0]\n",
    "        else:\n",
    "            a = self.sess.run(self.sample_test, {self.tfs: state, self.obs: observation})[0]\n",
    "        return np.clip(a, self.action_low, self.action_high)\n",
    "    \n",
    "    def remember(self, state, observation, action, reward, next_state, next_observation):\n",
    "        self.memory += [[state[0], observation[0], action, reward, next_state[0], next_observation[0]]]\n",
    "    \n",
    "    def discount_rewards(self, rewards, gamma, value_next=0.0):\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        running_add = value_next\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            discounted_r[t] = running_add = running_add * gamma + rewards[t]\n",
    "        return discounted_r\n",
    "    \n",
    "    def process_memory(self):\n",
    "        memory = np.vstack(np.array(self.memory))\n",
    "        states = np.vstack(memory[:,0])\n",
    "        observations = np.vstack(memory[:,1]).reshape((-1,16,40,1))\n",
    "        actions = np.vstack(memory[:,2])\n",
    "        rewards = memory[:,3]\n",
    "        last_next_state = memory[:,4][-1]\n",
    "        last_next_observation = memory[:,5][-1]\n",
    "        discounted_ep_rs = self.discount_rewards(rewards, self.gamma)[:, newaxis]\n",
    "        value_estimates = self.sess.run(self.v, {self.tfs: states, self.obs: observations}).flatten()\n",
    "#         value_estimates = self.sess.run(self.v, {self.tfs: np.r_[states, last_next_state[newaxis, :]], \n",
    "#                                                  self.obs: np.r_[observations, last_next_observation[newaxis, :]]}).flatten()\n",
    "#         last_value_estimate = self.sess.run(self.v, {self.tfs: , self.obs: })[0]\n",
    "        value_estimates = np.append(value_estimates, 0)\n",
    "        delta_t = rewards + self.gamma * value_estimates[1:] - value_estimates[:-1]\n",
    "        advs = self.discount_rewards(delta_t, self.gamma * self.lam)[:, newaxis] #gae\n",
    "        last = states.shape[0] #min(500, states.shape[0])\n",
    "        self.history['states'] += [states[-last:]]\n",
    "        self.history['observations'] += [observations[-last:]]\n",
    "        self.history['actions'] += [actions[-last:]]\n",
    "        self.history['discounted_rs'] += [discounted_ep_rs[-last:]]\n",
    "        self.history['advantages'] += [advs[-last:]]\n",
    "        self.memory = [] # empty the memory\n",
    "    \n",
    "    def replay(self):\n",
    "        assert self.actor_model_set, 'model not set!'\n",
    "        assert self.critic_model_set, 'critic model not set!'\n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        \n",
    "        s = np.vstack(self.history['states'])\n",
    "        ob = np.vstack(self.history['observations'])\n",
    "        ac = np.vstack(self.history['actions'])\n",
    "        dc_r = np.vstack(self.history['discounted_rs'])\n",
    "        ad = np.vstack(self.history['advantages'])\n",
    "#         ad = (ad-ad.mean())/ad.std()\n",
    "        \n",
    "        for _ in range(10): # update K epochs\n",
    "            s, ob, ac, dc_r, ad = shuffle(s, ob, ac, dc_r, ad)\n",
    "            for l in range(s.shape[0]//self.batch_size):\n",
    "                start = l * self.batch_size\n",
    "                end = (l + 1) * self.batch_size\n",
    "                self.sess.run(self.atrain_op, {self.tfs: s[start:end, :], self.obs:ob[start:end, :], self.tfa: ac[start:end, :], self.tfadv: ad[start:end, :]})\n",
    "                self.sess.run(self.ctrain_op, {self.tfs: s[start:end, :], self.obs:ob[start:end, :], self.tfdc_r: dc_r[start:end, :]})\n",
    "            if s.shape[0] % self.batch_size != 0:\n",
    "                res = s.shape[0] % self.batch_size\n",
    "                self.sess.run(self.atrain_op, {self.tfs: s[-res:, :], self.obs:ob[-res:, :], self.tfa: ac[-res:, :], self.tfadv: ad[-res:, :]})\n",
    "                self.sess.run(self.ctrain_op, {self.tfs: s[-res:, :], self.obs:ob[-res:, :], self.tfdc_r: dc_r[-res:, :]})\n",
    "#         self.actor_learning_rate *= self.learning_rate_decay\n",
    "#         self.critic_learning_rate *= self.learning_rate_decay\n",
    "        \n",
    "        for key in self.history:\n",
    "            self.history[key] = [] # empty the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_name = \"RL-AutonomousCar\"\n",
    "#file_name=env_name\n",
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[1]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edges(img, thr1=100, thr2=150):\n",
    "    imgBlur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    edges = cv2.Canny(np.uint8(imgBlur*255),thr1,thr2)/255\n",
    "    return edges.reshape(-1,*edges.shape,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showEdges(img, thr1=100, thr2=150):\n",
    "    imgBlur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    edges = cv2.Canny(np.uint8(imgBlur*255),thr1,thr2)/255\n",
    "    # gr = img[...,:3].dot([0.299, 0.587, 0.114])\n",
    "    plt.subplots(figsize=(15,15))\n",
    "    plt.subplot(131), plt.xticks([]), plt.yticks([]), plt.title('Original Image')\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(132), plt.xticks([]), plt.yticks([]), plt.title('Blurred Image')\n",
    "    plt.imshow(imgBlur)\n",
    "    plt.subplot(133), plt.xticks([]), plt.yticks([]), plt.title('Edges')\n",
    "    plt.imshow(edges, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "#env_info = env.reset(train_mode=False)[default_brain]\n",
    "env_info = env.reset()\n",
    "brainInfo = env_info['CarLearning']\n",
    "print(np.array(brainInfo.visual_observations).shape)\n",
    "#print(env_info.states)\n",
    "\n",
    "for observation in brainInfo.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])\n",
    "\n",
    "img = brainInfo.visual_observations[0][0]\n",
    "#plt.imshow(img)\n",
    "#plt.show()\n",
    "showEdges(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State is : [2.00821924]\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    for _ in range(10):\n",
    "        env_info = env.step([1,0])[default_brain]\n",
    "        #showEdges(env_info.visual_observations[0][0])\n",
    "#     print(str((i+1)*10)+' steps')\n",
    "    img = env_info.visual_observations[0][0]\n",
    "    if i==0:\n",
    "        sky = img[:3,:]\n",
    "    else:\n",
    "        sky += img[:3,:]\n",
    "#     showEdges(img)\n",
    "sky /= 40\n",
    "print('State is :', env_info.vector_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAA8CAYAAACdKPrlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x272c32baac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sky)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = PPOAgent(env,\n",
    "                n_actions=2,\n",
    "                n_features=1,\n",
    "                actor_learning_rate=1e-5,\n",
    "                critic_learning_rate=2e-5\n",
    "                )\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 405 rewards: 3820.00\r"
     ]
    }
   ],
   "source": [
    "# PPO\n",
    "n_episodes = 1000\n",
    "\n",
    "#agent.saver.restore(agent.sess, \"model/model2_ppo.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=True)[default_brain]\n",
    "    state = env_info.vector_observations\n",
    "    env_info.visual_observations[0][0][:3,:] = sky # pretend that we always see the sky above\n",
    "    observation = edges(env_info.visual_observations[0][0])\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state, observation)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        env_info.visual_observations[0][0][:3,:] = sky # pretend that we always see the sky above\n",
    "        next_observation = edges(env_info.visual_observations[0][0])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        agent.remember(state, observation, action, reward, next_state, next_observation)\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            agent.process_memory()\n",
    "            rewards += [r]\n",
    "            break\n",
    "    if (i_episode+1) % agent.agents == 0: # update every n_agent episodes\n",
    "        agent.replay()\n",
    "#     if (i_episode+1) % 100 == 0:\n",
    "#         agent.saver.save(agent.sess, \"model/model2_ppo.ckpt\");\n",
    "agent.saver.save(agent.sess, \"model/model2_ppo.ckpt\");\n",
    "print(\"\\n\")\n",
    "print(\"finished learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model2_ppo.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model2_ppo.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1 rewards: 355.80\r\n",
      "\n",
      "finished testing!\n"
     ]
    }
   ],
   "source": [
    "#ppo\n",
    "n_episodes = 1\n",
    "\n",
    "test_rewards = []\n",
    "agent.saver.restore(agent.sess, \"model/model2_ppo.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=False)[default_brain]\n",
    "    state = env_info.vector_observations\n",
    "    env_info.visual_observations[0][0][:3,:] = sky # pretend that we always see the sky above\n",
    "    observation = edges(env_info.visual_observations[0][0])\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state, observation)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        state = env_info.vector_observations\n",
    "        env_info.visual_observations[0][0][:3,:] = sky\n",
    "        observation = edges(env_info.visual_observations[0][0])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26a2ee24f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwi\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\ashwi\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "print(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b9c20199e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
